# ğŸ“š DocuMind: Intelligent Document Q&A System

An end-to-end RAG (Retrieval-Augmented Generation) application for querying technical documents and textbooks. Built with Google Gemini, LangChain, ChromaDB, and Streamlit for the **DataTalksClub LLM Zoomcamp**.

## ğŸ¯ Problem Description

Technical professionals and students struggle with information overload when searching through hundreds of pages of documentation and textbooks. **DocuMind** solves this by:
- Creating a searchable AI-powered knowledge base from PDF documents and text files
- Using semantic search to find relevant information instantly
- Providing accurate, context-aware answers with source citations

## âœ¨ Features

- ğŸ” **Intelligent Retrieval**: Semantic search using Google's text-embedding-004 model
- ğŸ¤– **AI Answers**: Google Gemini Pro generates comprehensive responses with source attribution
- ğŸ“Š **Advanced Monitoring**: 
  - **LangSmith Tracing**: Real-time LLM call tracking, token usage, latency analysis
  - **Grafana Dashboards**: Visual metrics for queries, response times, feedback
  - **ChromaDB Admin UI**: Vector database inspection and management
  - **Integrated Dashboard**: 7 Plotly charts for user feedback and performance
- ğŸ¨ **Modern UI**: Streamlit interface with Q&A and dashboard tabs
- ğŸ³ **Docker Ready**: Full containerization with docker-compose (4 services)
- ğŸ“ˆ **Optimized**: Evaluated 4 retrieval strategies and 4 prompt templates to select best approaches

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit  â”‚  User Interface
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangChain  â”‚  RAG Pipeline
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ChromaDBâ”‚ â”‚Geminiâ”‚
â”‚ Vector â”‚ â”‚ LLM  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Option 1: Local Setup

**Prerequisites:** Python 3.12+ and Google Gemini API Key ([Get one here](https://makersuite.google.com/app/apikey))

   ```bash
# 1. Clone and navigate
git clone <your-repo-url>
   cd rag_project

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
   pip install -r requirements.txt

# 4. Configure environment
echo "GOOGLE_API_KEY=your_api_key_here" > .env

# 5. Add your documents
mkdir -p data
# Copy your PDFs/text files to data/ folder

# 6. Run the app
   streamlit run app.py
   ```

Access at http://localhost:8501

### Option 2: Docker Deployment (Full Monitoring Stack)

```bash
# 1. Create .env file with your API keys
cat > .env << EOF
GOOGLE_API_KEY=your_api_key_here
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_PROJECT=documind-rag
GRAFANA_USER=admin
GRAFANA_PASSWORD=admin
EOF

# 2. Build and run all services
docker-compose up -d

# 3. Access services:
# Main App:        http://localhost:8501
# Grafana:         http://localhost:3000
# ChromaDB UI:     http://localhost:8000
# Prometheus:      http://localhost:9090
# LangSmith:       https://smith.langchain.com

# View logs
docker-compose logs -f rag-app

# Stop all services
docker-compose down

# Full cleanup (removes volumes)
docker-compose down -v
```

**Docker Stack (4 Services):**
- âœ… **Main RAG App**: Streamlit UI on port 8501
- âœ… **Grafana**: Metrics visualization on port 3000
- âœ… **Prometheus**: Metrics collection on port 9090
- âœ… **ChromaDB Admin**: Vector DB UI on port 8000
- âœ… Named volumes for data persistence
- âœ… Resource limits (2 CPU cores, 4GB RAM)
- âœ… Health checks and security (non-root user)

### Option 3: Local Monitoring Stack

Run monitoring tools locally alongside your main app:

**1. LangSmith Tracing (LLM Monitoring)**

```bash
# Add to .env file
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_PROJECT=documind-rag

# Run your app - tracing is automatic
streamlit run app.py
```
View traces at: https://smith.langchain.com/projects

**2. Monitoring Stack with Docker (Recommended)**

Run Grafana and Prometheus for monitoring:

```bash
# Start monitoring stack (Grafana + Prometheus + ChromaDB Server)
docker-compose -f docker-compose-monitoring.yml up -d

# Access services:
# Grafana:            http://localhost:3000 (admin/admin)
# Prometheus:         http://localhost:9090
# ChromaDB Server:    http://localhost:8000

# Stop monitoring stack
docker-compose -f docker-compose-monitoring.yml down
```

**3. ChromaDB Viewer (Custom UI)**

For inspecting your vector database:

```bash
# Run alongside your app (separate terminal)
streamlit run chromadb_viewer.py --server.port 8001
```

Access at: http://localhost:8001

**Features:**
- ğŸ” Search vector database with similarity scores
- ğŸ“„ Browse documents with full metadata
- ğŸ“Š Collection statistics and analytics
- ğŸ“¥ Export data as CSV
- ğŸ—‚ï¸ Multi-collection support

## ğŸ“– Usage

### Main Application

1. **Auto-Initialization**: System automatically loads on startup
   - Detects existing vector database or processes documents from `data/` folder
   - First-time processing may take a few minutes

2. **Ask Questions**:
   - Type questions like "What is the Floyd-Warshall algorithm?"
   - Get AI-generated answers with source citations
   - View retrieved document chunks

3. **Provide Feedback**:
   - Click ğŸ‘ or ğŸ‘ after each answer
   - Feedback is logged for analytics

4. **Monitor Performance**:
   - Switch to "ğŸ“Š Monitoring Dashboard" tab
   - View 7 charts: feedback distribution, response times, query volume, etc.
   - Export data as CSV

### Loading New Documents

```bash
# Add files to data/ folder
cp new_document.pdf data/

# Remove old vector store
rm -rf chroma_db

# Restart app (it will auto-reinitialize)
streamlit run app.py
```

### Running Evaluations

**Retrieval Evaluation** (compare 4 search strategies):
```bash
python retrieval_evaluation.py
# Results saved to retrieval_evaluation_results.json
```

**LLM Evaluation** (compare 4 prompt templates):
```bash
python llm_evaluation.py
# Results saved to llm_evaluation_results.json
```

## ğŸ”¬ Evaluation Results

### Retrieval Optimization

Tested 4 approaches on 5 diverse queries:

| Approach | Precision | Keyword Score | Combined |
|----------|-----------|---------------|----------|
| **Semantic (k=3)** â­ | **86.67%** | **2.53** | **1.37** |
| Semantic (k=5) | 84.00% | 2.36 | 1.30 |
| Semantic (k=10) | 84.00% | 2.16 | 1.24 |
| MMR (k=5) | 84.00% | 1.92 | 1.16 |

**Selected**: Semantic search with k=3 for highest precision and relevance.

**Scripts**: [`retrieval_evaluation.py`](retrieval_evaluation.py) | [`retrieval_evaluation_results.json`](retrieval_evaluation_results.json)

### LLM Prompt Optimization

Tested 4 prompt templates on 5 queries (including unknown topics):

| Prompt Template | Quality Score | Avg Words | Notes |
|----------------|---------------|-----------|-------|
| **Expert Technical** â­ | **8.00/10** | **701** | Comprehensive, technical |
| Detailed Context | 7.90/10 | 354 | Balanced detail |
| Structured | 7.00/10 | 129 | Organized, moderate |
| Concise | 6.20/10 | 50 | Brief, minimal |

**Selected**: Expert Technical Style for highest quality and proper handling of unknowns.

**Scripts**: [`llm_evaluation.py`](llm_evaluation.py) | [`llm_evaluation_results.json`](llm_evaluation_results.json)

## ğŸ“Š Monitoring & Observability

### Overview: 4-Layer Monitoring Stack

This project implements comprehensive monitoring at multiple levels:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. LangSmith â†’ LLM call tracing            â”‚ (Primary)
â”‚  2. Grafana â†’ System metrics visualization  â”‚
â”‚  3. ChromaDB Admin â†’ Vector DB inspection   â”‚
â”‚  4. Streamlit â†’ User feedback analytics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. LangSmith Tracing (Primary LLM Monitoring) â­

**Production-grade LLM observability** - the most important monitoring tool for RAG systems:

**What it monitors:**
- ğŸ” **Request Tracing**: Every LLM call with full inputs/outputs
- â±ï¸ **Latency Breakdown**: Embedding â†’ Retrieval â†’ Generation timing
- ğŸ’° **Cost Monitoring**: Token usage (input/output) and API costs per query
- ğŸ› **Debugging**: Detailed chain execution with intermediate steps
- ğŸ“ˆ **Analytics**: Query patterns, failure rates, performance trends
- ğŸ§ª **A/B Testing**: Compare different prompts and retrieval strategies

**How to enable:**
```bash
# Add to .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls__your_api_key_here
LANGCHAIN_PROJECT=documind-rag
```

**Access:** https://smith.langchain.com/projects

**Why it's essential:** Unlike basic metrics, LangSmith shows *why* your RAG system behaves the way it does - what context was retrieved, how the LLM interpreted it, and where bottlenecks occur.

### 2. Grafana Dashboards (System Metrics)

Visual monitoring with Prometheus + Grafana stack (auto-configured in Docker):

**Pre-built dashboard includes:**
- Query rate and volume trends
- Response time percentiles (p50, p95)
- Feedback distribution (positive/negative)
- Sources retrieved per query
- System health metrics

**Access:** http://localhost:3000 (default: admin/admin)

**Use case:** High-level system health and performance trends over time.

### 3. ChromaDB Viewer UI (Vector Database)

Custom Streamlit interface to inspect and manage your vector embeddings:
- View collections and document counts (15,354 chunks)
- **Search with similarity scores**: Query the vector DB directly
- Browse documents with metadata (source, page, content)
- View statistics: document lengths, source distribution
- Export data as CSV for analysis
- Debug retrieval issues (why certain docs are/aren't retrieved)

**Access:** http://localhost:8001 (run `streamlit run chromadb_viewer.py --server.port 8001`)

**Use case:** Debug retrieval problems, understand document distribution, verify embeddings, test queries.

### 4. Integrated Streamlit Dashboard

Built-in analytics with 7 interactive Plotly charts:

1. **Feedback Distribution** - Pie chart of positive/negative feedback
2. **Feedback Timeline** - Trend analysis over time
3. **Response Time** - Histogram of query processing speeds
4. **Query Volume** - Daily usage bar chart
5. **Sources Retrieved** - Distribution of retrieved documents
6. **Answer Length** - Box plot of response sizes
7. **Hourly Activity** - Heatmap of peak usage times

**Features**:
- User feedback collection (ğŸ‘/ğŸ‘ buttons)
- Key metrics: total queries, positive %, avg response time, avg sources
- CSV data export for external analysis

**Access:** Main app â†’ "ğŸ“Š Monitoring Dashboard" tab

**Use case:** End-user feedback analysis, understand query patterns, identify improvement areas.

## ğŸ“ Project Structure

```
rag_project/
â”œâ”€â”€ app.py                              # Streamlit UI (integrated dashboard)
â”œâ”€â”€ rag_pipeline.py                     # Core RAG implementation
â”œâ”€â”€ document_processor.py               # PDF/text processing
â”œâ”€â”€ feedback_storage.py                 # User feedback & interaction logging
â”œâ”€â”€ retrieval_evaluation.py             # Retrieval evaluation script
â”œâ”€â”€ llm_evaluation.py                   # LLM prompt evaluation script
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ .env                                # Environment configuration
â”œâ”€â”€ Dockerfile                          # Container definition
â”œâ”€â”€ docker-compose.yml                  # Container orchestration (4 services)
â”œâ”€â”€ prometheus.yml                      # Prometheus configuration
â”œâ”€â”€ grafana-datasources.yml             # Grafana data sources
â”œâ”€â”€ grafana-dashboards.yml              # Grafana dashboard provisioning
â”œâ”€â”€ grafana-dashboard.json              # Pre-built Grafana dashboard
â”œâ”€â”€ data/                               # Document storage (your PDFs/TXTs)
â”œâ”€â”€ chroma_db/                          # Vector database (auto-generated)
â””â”€â”€ feedback_data.json                  # Analytics data (auto-generated)
```

## ğŸ› ï¸ Technologies

- **LLM**: Google Gemini 2.5 Pro
- **Embeddings**: text-embedding-004
- **Vector DB**: ChromaDB (with Admin UI)
- **Framework**: LangChain (with LangSmith tracing)
- **Interface**: Streamlit
- **Monitoring**: Prometheus + Grafana + LangSmith
- **Containerization**: Docker & Docker Compose
- **Analytics**: Plotly for visualizations
- **Language**: Python 3.12

## ğŸ“Š System Performance

- **Processing Speed**: ~1,400 chunks/minute
- **Retrieval Speed**: < 1 second
- **Batch Capacity**: 5,000 docs/batch, 15,000+ total
- **Formats**: PDF, TXT
- **Sample Dataset**: 10+ technical books, 15,354 chunks

## ğŸ”® Future Enhancements

- Hybrid search combining vector and keyword approaches
- Query rewriting for improved retrieval
- Re-ranking with cross-encoder
- Multi-modal support for images and tables
- Cloud deployment (AWS/GCP/Azure)
- FastAPI REST API
- Multi-language document support
- Conversational memory for follow-up questions

---

## ğŸ“¸ Screenshots

![Screenshot 1](images/Screenshot%20from%202025-10-20%2009-22-39.png)

![Screenshot 2](images/Screenshot%20from%202025-10-20%2009-23-01.png)

![Screenshot 3](images/Screenshot%20from%202025-10-20%2009-25-04.png)

![Screenshot 4](images/Screenshot%20from%202025-10-20%2009-27-55.png)

![Screenshot 5](images/Screenshot%20from%202025-10-20%2009-46-42.png)

![Screenshot 6](images/Screenshot%20from%202025-10-20%2011-16-20.png)

---